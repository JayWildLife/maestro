{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Overview Maestro is a game room orchestration system, which can be used by multiplayer games to manage the scalability of its game rooms (Aka game servers). Ideally, Maestro should be used by games that implement dedicated game server (dgs) architecture. Each game room is a dedicated game server that runs in a match execution context, a group of game rooms is organized in a Scheduler . Each time that a user requires some change in the Scheduler or game room, Maestro creates an Operation that is a change unit that will be enqueued and handled sequentially by a proper worker related exclusively to its respective Scheduler, in other words, each Scheduler has a worker that sequentially handles Operations that is created to it. A Scheduler defines the requirements of a game room as to how much memory and CPU it will need to execute and other information. Each Operation has its own properties to be executed. Maestro has five components: Management API, Game Rooms API, Execution Worker, Runtime Watcher and Metrics Reporter. They all deliver together the Maestro features that will be described in the next sections. Features & Components Maestro is composed by: Management API: this is the component that the users will use to create your requests to interact with Maestro. For example: create a Scheduler, get Scheduler information, etc. Execution Worker: have an execution component to handle Operations to each Scheduler. For example: three Schedulers will have each one an execution component. Game Rooms API : game rooms API exposes an HTTP API or a GRPc service to receive game rooms messages that symbolize their respective status. For example: when a game room is ready to receive matches it will send a message informing that. Runtime Watcher: is a component in Maestro that listen to Runtime events and reflect them in Maestro. For example: when a game room was created it is notified that this event happened. Metrics Reporter: time spaced this component query the Runtime for metrics related to rooms and expose them in an open metrics route. For example: how many occupied rooms and ready rooms are up.","title":"Home"},{"location":"#overview","text":"Maestro is a game room orchestration system, which can be used by multiplayer games to manage the scalability of its game rooms (Aka game servers). Ideally, Maestro should be used by games that implement dedicated game server (dgs) architecture. Each game room is a dedicated game server that runs in a match execution context, a group of game rooms is organized in a Scheduler . Each time that a user requires some change in the Scheduler or game room, Maestro creates an Operation that is a change unit that will be enqueued and handled sequentially by a proper worker related exclusively to its respective Scheduler, in other words, each Scheduler has a worker that sequentially handles Operations that is created to it. A Scheduler defines the requirements of a game room as to how much memory and CPU it will need to execute and other information. Each Operation has its own properties to be executed. Maestro has five components: Management API, Game Rooms API, Execution Worker, Runtime Watcher and Metrics Reporter. They all deliver together the Maestro features that will be described in the next sections.","title":"Overview"},{"location":"#features-components","text":"Maestro is composed by: Management API: this is the component that the users will use to create your requests to interact with Maestro. For example: create a Scheduler, get Scheduler information, etc. Execution Worker: have an execution component to handle Operations to each Scheduler. For example: three Schedulers will have each one an execution component. Game Rooms API : game rooms API exposes an HTTP API or a GRPc service to receive game rooms messages that symbolize their respective status. For example: when a game room is ready to receive matches it will send a message informing that. Runtime Watcher: is a component in Maestro that listen to Runtime events and reflect them in Maestro. For example: when a game room was created it is notified that this event happened. Metrics Reporter: time spaced this component query the Runtime for metrics related to rooms and expose them in an open metrics route. For example: how many occupied rooms and ready rooms are up.","title":"Features &amp; Components"},{"location":"reference/","text":"Reference The reference documentation are available for this module: Maestro Architecture Open API Schedulers Operations","title":"Reference"},{"location":"reference/#reference","text":"The reference documentation are available for this module: Maestro Architecture Open API Schedulers Operations","title":"Reference"},{"location":"tutorials/","text":"Tutorials The following tutorials are available for this module: Getting Started Guide Configuring Scheduler Autoscaling Configuring Events Forwarding Development","title":"Tutorials"},{"location":"tutorials/#tutorials","text":"The following tutorials are available for this module: Getting Started Guide Configuring Scheduler Autoscaling Configuring Events Forwarding Development","title":"Tutorials"},{"location":"how-to-guides/Guidelines/","text":"Assumptions, Dos & Don'ts Describe common assumptions, or actions that you want your users to avoid. If there are multiple ways to solve problems using your application, you can document here the best options you have identified. Lessons Learned & Best Practices You can fill this section with best practices and enrich it as you learn better practices for the use of your application.","title":"Guidelines"},{"location":"how-to-guides/Guidelines/#assumptions-dos-donts","text":"Describe common assumptions, or actions that you want your users to avoid. If there are multiple ways to solve problems using your application, you can document here the best options you have identified.","title":"Assumptions, Dos &amp; Don'ts"},{"location":"how-to-guides/Guidelines/#lessons-learned-best-practices","text":"You can fill this section with best practices and enrich it as you learn better practices for the use of your application.","title":"Lessons Learned &amp; Best Practices"},{"location":"how-to-guides/User%20Guides/","text":"Features & Components Short list of the components, features or elements that the user needs to know to make the best of your application. On this section, you can include any guides and how tos that involve interaction with the UI in RING. Navigation & How To Guides Describe any menus and buttons in the user interface not covered in the previous section Include how users can interact with the features on your interface to get the desired results. Troubleshooting Document here any known errors, workarounds for bugs, troubleshooting details, and tips and tricks to avoid common mistakes.","title":"UserGuides"},{"location":"how-to-guides/User%20Guides/#features-components","text":"Short list of the components, features or elements that the user needs to know to make the best of your application. On this section, you can include any guides and how tos that involve interaction with the UI in RING.","title":"Features &amp; Components"},{"location":"how-to-guides/User%20Guides/#navigation-how-to-guides","text":"Describe any menus and buttons in the user interface not covered in the previous section Include how users can interact with the features on your interface to get the desired results.","title":"Navigation &amp; How To Guides"},{"location":"how-to-guides/User%20Guides/#troubleshooting","text":"Document here any known errors, workarounds for bugs, troubleshooting details, and tips and tricks to avoid common mistakes.","title":"Troubleshooting"},{"location":"reference/Architecture/","text":"Maestro Next is a composition of different modules. Internally they are all part of the same code base but could be executed by giving the right arguments to the command line (or to your docker container entry point). E.g. go run main.go start [MODULE_NAME] Maestro is composed of Management API , Rooms API , Operation Execution Worker , Runtime Watcher Worker , and Metrics Reporter Worker. Each module has its responsibilities and is divided apart in a way to avoid mixing the execution process. Each module was thought to avoid parallel problems and to give the client more visibility about which Operations are being executed and their respective status. Maestro modules Note: Maestro currently only supports Kubernetes as Game Rooms runtime system. So Workers interact with them. Management API Management API is the module responsible for receiving user requests. It accepts gRPC and HTTP requests and provides several kinds of routes that are aggregated in two services: schedulers service and operations service . The schedulers service exposes features for managing schedulers, like creating a new scheduler, fetching its information, or updating them. The operations service exposes features for tracking operations and changing their status, like listing operations by status or canceling them. Management API relies on Redis for retrieving operations and game rooms, and on Postgres for retrieving and persisting schedulers. Rooms API Rooms API is the module that provides an API that must be used by game rooms to sync their status with Maestro. To maestro work properly, it needs to be constantly informed about the status of each game room it manages. Also, if there are forwarders configured for the scheduler, those events are forwarded from Maestro at this module. Note: The requests that Maestro forwards in the Rooms API are documented in this proto file . Note: Maestro client could be used to ease the integration of the Game Room with Maestro. Operation Execution Worker Note: In Maestro a worker is a collection of routines that executes a flow related to one and only one Scheduler each. Operation Execution Worker is a process that constantly keeps ensuring each active Scheduler will have a thread (execution worker) that executes operations enqueued in the related Scheduler operation queue. So in this way became possible to track the events that happened and change a certain Scheduler in a healthier way. You could find all operations at Operations section Runtime Watcher Worker Note: In Maestro a worker is a collection of routines that executes a flow related to one and only one Scheduler each. Runtime Watcher Worker listens to runtime events related to the Scheduler and reflects the changes in Maestro . Currently, it listens for Game Rooms creation, deletion, and update. Metrics Reporter Worker Note: In Maestro a worker is a collection of routines that executes a flow related to one and only one Scheduler each. From time to time Metrics Reporter Worker watch runtime to report metrics from them, such as the number of game rooms instances that are ready , pending , error , unknown , or terminating status. As well it watches from Game Rooms storage its status that could be ready , pending , error , occupied , terminating , or unready . This module is optional since you don't need it for any specific functionalities of the application.","title":"Architecture"},{"location":"reference/Architecture/#maestro-modules","text":"Note: Maestro currently only supports Kubernetes as Game Rooms runtime system. So Workers interact with them.","title":"Maestro modules"},{"location":"reference/Architecture/#management-api","text":"Management API is the module responsible for receiving user requests. It accepts gRPC and HTTP requests and provides several kinds of routes that are aggregated in two services: schedulers service and operations service . The schedulers service exposes features for managing schedulers, like creating a new scheduler, fetching its information, or updating them. The operations service exposes features for tracking operations and changing their status, like listing operations by status or canceling them. Management API relies on Redis for retrieving operations and game rooms, and on Postgres for retrieving and persisting schedulers.","title":"Management API"},{"location":"reference/Architecture/#rooms-api","text":"Rooms API is the module that provides an API that must be used by game rooms to sync their status with Maestro. To maestro work properly, it needs to be constantly informed about the status of each game room it manages. Also, if there are forwarders configured for the scheduler, those events are forwarded from Maestro at this module. Note: The requests that Maestro forwards in the Rooms API are documented in this proto file . Note: Maestro client could be used to ease the integration of the Game Room with Maestro.","title":"Rooms API"},{"location":"reference/Architecture/#operation-execution-worker","text":"Note: In Maestro a worker is a collection of routines that executes a flow related to one and only one Scheduler each. Operation Execution Worker is a process that constantly keeps ensuring each active Scheduler will have a thread (execution worker) that executes operations enqueued in the related Scheduler operation queue. So in this way became possible to track the events that happened and change a certain Scheduler in a healthier way. You could find all operations at Operations section","title":"Operation Execution Worker"},{"location":"reference/Architecture/#runtime-watcher-worker","text":"Note: In Maestro a worker is a collection of routines that executes a flow related to one and only one Scheduler each. Runtime Watcher Worker listens to runtime events related to the Scheduler and reflects the changes in Maestro . Currently, it listens for Game Rooms creation, deletion, and update.","title":"Runtime Watcher Worker"},{"location":"reference/Architecture/#metrics-reporter-worker","text":"Note: In Maestro a worker is a collection of routines that executes a flow related to one and only one Scheduler each. From time to time Metrics Reporter Worker watch runtime to report metrics from them, such as the number of game rooms instances that are ready , pending , error , unknown , or terminating status. As well it watches from Game Rooms storage its status that could be ready , pending , error , occupied , terminating , or unready . This module is optional since you don't need it for any specific functionalities of the application.","title":"Metrics Reporter Worker"},{"location":"reference/Kubernetes/","text":"Kubernetes usage Maestro uses kubernetes for orchestrating game room instances. It uses a unique namespace for each scheduler, and a unique pod for each game room instance. We use client-go for communicating with kubernetes. The Runtime port is the interface used for managing resources, you can find all of the features we are using for managing k8s resources in it. The diagram below shows how maestro components interact with kubernetes for managing resources. flowchart BT classDef borderless stroke-width:0px classDef darkBlue fill:#00008B, color:#fff classDef brightBlue fill:#6082B6, color:#fff classDef gray fill:#62524F, color:#fff classDef gray2 fill:#4F625B, color:#fff subgraph maestroSystem[ ] subgraph k8s[ ] A3[Kubernetes] end class k8s,A3 brightBlue class A3, borderless subgraph WORKER[ ] A7[Operation Execution Worker<br/><br/>manage kubernetes resources by creating/deleting/updating pods abnd namespaces] end class WORKER,A7 brightBlue class WORKER,A7 borderless WORKER--Create namespace<br/>HTTPS-->k8s WORKER--Delete namespace<br/>HTTPS-->k8s WORKER--Create pod<br/>HTTPS-->k8s WORKER--Delete pod<br/>HTTPS-->k8s subgraph RUNTIME_WATCHER[ ] A8[Runtime Watcher <br/><br/> watch for change events in managed pods] end class RUNTIME_WATCHER,A8 brightBlue class RUNTIME_WATCHER,A8 borderless RUNTIME_WATCHER--List/Watch pods<br/>HTTPS-->k8s end click A3 \"/csymapp/mermaid-c4-model/blob/master/AWAComponent.md\" \"AWA\" Runtime watcher The runtime watcher component maintains a worker process for each scheduler that keeps watching and processing change events in pods resources. For doing that, it uses a pods informer , binding handlers for add , update and delete events for all pods managed by it. This component is not responsible for updating/creating/deleting kubernetes resources, all it does is to watch for changes and update its game room instances internal representation using redis. Operation execution worker The worker uses kubernetes for managing pods and namespaces. It executes several operations that, alongside other side effects, will need to create, update, and delete namespaces and pods. Currently, maestro does not check for HostPort conflict while creating new rooms One important note regarding how maestro creates pods: each new requested game room instance will be assigned to a pseudo-random port to be used as HostPort . Maestro uses the scheduler PortRange to generate the pseudo-random port. Currently, maestro does not check for HostPort conflict while creating new rooms. The final address of the game room will be composed of the Node address and the game room container assigned HostPort . That's the reason why maestro needs access for reading the Node addresses. Configuring cluster access Maestro needs the following permissions for managing resources in a kubernetes cluster: - nodes: read (we need to use the node address to compose the game room address); - pods: read, create, update, delete; - namespace: read, create, update, delete. Maestro provides two ways for configuring kubernetes cluster access. Using inCluster mode Set adapters.runtime.kubernetes.inCluster config value to true or use its env var equivalent, the kubernetes client will be configured automatically using the same service account of the maestro component running pod. This mode is recommended to be used when running maestro components in the same cluster in which the schedulers and rooms will be managed. Using kubeconfig mode Populate adapters.runtime.kubernetes.kubeconfig and adapters.runtime.kubernetes.masterUrl configs or use its env var equivalent, the kubernetes client will be configured using the provided kubeconfig file and master url.","title":"Kubernetes Usage"},{"location":"reference/Kubernetes/#kubernetes-usage","text":"Maestro uses kubernetes for orchestrating game room instances. It uses a unique namespace for each scheduler, and a unique pod for each game room instance. We use client-go for communicating with kubernetes. The Runtime port is the interface used for managing resources, you can find all of the features we are using for managing k8s resources in it. The diagram below shows how maestro components interact with kubernetes for managing resources. flowchart BT classDef borderless stroke-width:0px classDef darkBlue fill:#00008B, color:#fff classDef brightBlue fill:#6082B6, color:#fff classDef gray fill:#62524F, color:#fff classDef gray2 fill:#4F625B, color:#fff subgraph maestroSystem[ ] subgraph k8s[ ] A3[Kubernetes] end class k8s,A3 brightBlue class A3, borderless subgraph WORKER[ ] A7[Operation Execution Worker<br/><br/>manage kubernetes resources by creating/deleting/updating pods abnd namespaces] end class WORKER,A7 brightBlue class WORKER,A7 borderless WORKER--Create namespace<br/>HTTPS-->k8s WORKER--Delete namespace<br/>HTTPS-->k8s WORKER--Create pod<br/>HTTPS-->k8s WORKER--Delete pod<br/>HTTPS-->k8s subgraph RUNTIME_WATCHER[ ] A8[Runtime Watcher <br/><br/> watch for change events in managed pods] end class RUNTIME_WATCHER,A8 brightBlue class RUNTIME_WATCHER,A8 borderless RUNTIME_WATCHER--List/Watch pods<br/>HTTPS-->k8s end click A3 \"/csymapp/mermaid-c4-model/blob/master/AWAComponent.md\" \"AWA\"","title":"Kubernetes usage"},{"location":"reference/Kubernetes/#runtime-watcher","text":"The runtime watcher component maintains a worker process for each scheduler that keeps watching and processing change events in pods resources. For doing that, it uses a pods informer , binding handlers for add , update and delete events for all pods managed by it. This component is not responsible for updating/creating/deleting kubernetes resources, all it does is to watch for changes and update its game room instances internal representation using redis.","title":"Runtime watcher"},{"location":"reference/Kubernetes/#operation-execution-worker","text":"The worker uses kubernetes for managing pods and namespaces. It executes several operations that, alongside other side effects, will need to create, update, and delete namespaces and pods. Currently, maestro does not check for HostPort conflict while creating new rooms One important note regarding how maestro creates pods: each new requested game room instance will be assigned to a pseudo-random port to be used as HostPort . Maestro uses the scheduler PortRange to generate the pseudo-random port. Currently, maestro does not check for HostPort conflict while creating new rooms. The final address of the game room will be composed of the Node address and the game room container assigned HostPort . That's the reason why maestro needs access for reading the Node addresses.","title":"Operation execution worker"},{"location":"reference/Kubernetes/#configuring-cluster-access","text":"Maestro needs the following permissions for managing resources in a kubernetes cluster: - nodes: read (we need to use the node address to compose the game room address); - pods: read, create, update, delete; - namespace: read, create, update, delete. Maestro provides two ways for configuring kubernetes cluster access.","title":"Configuring cluster access"},{"location":"reference/Kubernetes/#using-incluster-mode","text":"Set adapters.runtime.kubernetes.inCluster config value to true or use its env var equivalent, the kubernetes client will be configured automatically using the same service account of the maestro component running pod. This mode is recommended to be used when running maestro components in the same cluster in which the schedulers and rooms will be managed.","title":"Using inCluster mode"},{"location":"reference/Kubernetes/#using-kubeconfig-mode","text":"Populate adapters.runtime.kubernetes.kubeconfig and adapters.runtime.kubernetes.masterUrl configs or use its env var equivalent, the kubernetes client will be configured using the provided kubeconfig file and master url.","title":"Using kubeconfig mode"},{"location":"reference/OpenAPI/","text":"const ui = SwaggerUIBundle({ url: 'https://raw.githubusercontent.com/topfreegames/maestro/main/proto/apidocs.swagger.json', dom_id: '#swagger-ui', })","title":"OpenAPI"},{"location":"reference/Operations/","text":"What is Operation is a core concept at Maestro, and it represents executions done in multiple layers of Maestro, a state update, or a configuration change. Operations can be created by user actions while managing schedulers (e.g. consuming management API), or internally by Maestro to fulfill internal states requirements Operations are heavily inspired by the Command Design Pattern Definition and executors Maestro will have multiple operations, and those will be set using pairs of definitions and executors. An operation definition consists of the operation parameters. An operation executor is where the actual operation execution and rollback logic is implemented, and it will receive as input its correlated definition. So, for example, the CreateSchedulerExecutor will always receive a CreateSchedulerDefinition . flowchart TD subgraph operations [Operations] subgraph operation_implementation [Operation Impl.] definition(Definition) executor(Executor) end subgraph operation_implementation2 [Operation Impl.] definition2(Definition) executor2(Executor) end subgraph operation_implementation3 [Operation Impl.] definition3(Definition) executor3(Executor) end ... end Operation Structure id : Unique operation identification. Auto-Generated; status : Operations status. For reference, see here . definitionName : Name of the operation. For reference, see here . schedulerName : Name of the scheduler which this operation affects. createdAt : Timestamp representing when the operation was enqueued. input : Contains the input value for this operation. Each operation has its own input format. For details, see below . executionHistory : Contains logs with detailed info about the operation execution. See below . id: String status: String definitionName: String schedulerName: String createdAt: Timestamp input: Any executionHistory: ExecutionHistory Input Create Scheduler scheduler: Scheduler Create New Scheduler Version scheduler: Scheduler Switch Scheduler Version newActiveVersion: Scheduler Add Rooms amount: Integer Remove Rooms amount: Integer Execution History createdAt: timestamp event: String createdAt : When did the event happened. event : What happened. E.g. \"Operation failed because...\". How does Maestro handle operations Each scheduler has 1 operation execution (no operations running in parallel for a scheduler). Every operation execution has 1 queue for pending operations. When the worker is ready to work on a new operation, it'll pop from the queue. The operation is executed by the worker following the lifecycle described here . State An operation can have one of the Status below: Pending : When an operation is enqueued to be executed; Evicted : When an operation is unknown or should not be executed By Maestro; In Progress : Operation is currently being executed; Finished : Operation finished; Execution succeeded; Error : Operation finished. Execution failed; Canceled : Operation was canceled by the user. State Machine flowchart TD pending(Pending) in_progress(In Progress) evicted(Evicted) finished(Finished) canceled(Canceled) error(Error) pending --> in_progress; pending --> evicted; in_progress --> finished; in_progress --> error; in_progress --> canceled; Lifecycle flowchart TD finish((End)) created(\"Created (Pending)\") evicted(Evicted) error(Error) finished(Finished) canceled(Canceled) should_execute{Should Execute?} execution_succeeded{Success?} err_kind{Error Kind} execute[[Execute]] rollback[[Rollback]] canceled_by_user>Canceled By User] created --> should_execute; should_execute -- No --> evicted --> finish; should_execute -- Yes --> execute; execute --> execution_succeeded; execute --> canceled_by_user --> rollback; execution_succeeded -- Yes --> finished --> finish; execution_succeeded -- No --> rollback; rollback --> err_kind; err_kind -- Canceled --> canceled --> finish; err_kind -- Error --> error --> finish Lease What is the operation lease Lease is a mechanism to track the operations' execution process and check if we can rely on the current/future operation state. Why Operations have it Sometimes, an operation might get stuck. It could happen, for example, if the worker crashes during the execution of an operation. To keep track of operations, we assign each operation a Lease. This Lease has a TTL (time to live). When the operation is being executed, this TTL is renewed each time the lease is about to expire while the operation is still in progress. It'll be revoked once the operation is finished. Troubleshooting If an operation is fetched and the TTL expired (the TTL is in the past), the operation probably got stuck, and we can't rely upon its current state, nor guarantee the required side effects of the execution or rollback have succeeded. If an operation does not have a Lease, it either did not start at all (should be on the queue) or is already finished. An Active Operation without a Lease is at an invalid state. Maestro do not have a self-healing routine yet for expired operations. Operation Lease Lifecycle flowchart TD finish_routine((End)) start_routine((Start)) finish((End)) operation_finished{Op. Finished?} to_execute[Operation to Execute] grant_lease[[Grant Lease]] renew_lease[[Renew Lease]] revoke_lease[[Revoke Lease]] wait_for_ttl(Wait for TTL) execute(Execute Operation) to_execute --> grant_lease; grant_lease --> renew_lease_routine; grant_lease --> execute; subgraph renew_lease_routine [ASYNC Renew Lease Routine] start_routine --> wait_for_ttl; wait_for_ttl --> operation_finished; operation_finished -- Yes --> revoke_lease; operation_finished -- No --> renew_lease --> wait_for_ttl; revoke_lease --> finish_routine; end renew_lease_routine --> finish; Available Operations For more details on how to use Maestro API, see this section . Create Scheduler Accessed through the POST /schedulers endpoint. Creates the scheduler structure for receiving rooms; The scheduler structure is validated, but the game room is not; If operation fails, rollback feature will delete anything created related to scheduler. Create New Scheduler Version Accessed through the POST /schedulers/:schedulerName endpoint. Creates a validation room (deleted right after). If Maestro cannot receive pings (not forwarded) from validation game room, operation fails; When this operation finishes successfully, it enqueues the \"Switch Active Version\". If operation fails rollback routine deletes anything (except for the operation) created related to new version. Switch Active Version Accessed through PUT /schedulers/:schedulerName endpoint. If it's a major change (anything under Scheduler.Spec changed), GRUs are replaced using scheduler maxSurge property; If it's a minor change (Scheduler.Spec haven't changed), GRUs are not replaced; Add Rooms Accessed through POST /schedulers/:schedulerName/add-rooms endpoint. If any room fail on creating, the operation fails and created rooms are deleted on rollback feature; Remove Rooms Accessed through POST /schedulers/:schedulerName/remove-rooms endpoint. Remove rooms based on amount; Rollback routine does nothing.","title":"Operations"},{"location":"reference/Operations/#what-is","text":"Operation is a core concept at Maestro, and it represents executions done in multiple layers of Maestro, a state update, or a configuration change. Operations can be created by user actions while managing schedulers (e.g. consuming management API), or internally by Maestro to fulfill internal states requirements Operations are heavily inspired by the Command Design Pattern","title":"What is"},{"location":"reference/Operations/#definition-and-executors","text":"Maestro will have multiple operations, and those will be set using pairs of definitions and executors. An operation definition consists of the operation parameters. An operation executor is where the actual operation execution and rollback logic is implemented, and it will receive as input its correlated definition. So, for example, the CreateSchedulerExecutor will always receive a CreateSchedulerDefinition . flowchart TD subgraph operations [Operations] subgraph operation_implementation [Operation Impl.] definition(Definition) executor(Executor) end subgraph operation_implementation2 [Operation Impl.] definition2(Definition) executor2(Executor) end subgraph operation_implementation3 [Operation Impl.] definition3(Definition) executor3(Executor) end ... end","title":"Definition and executors"},{"location":"reference/Operations/#operation-structure","text":"id : Unique operation identification. Auto-Generated; status : Operations status. For reference, see here . definitionName : Name of the operation. For reference, see here . schedulerName : Name of the scheduler which this operation affects. createdAt : Timestamp representing when the operation was enqueued. input : Contains the input value for this operation. Each operation has its own input format. For details, see below . executionHistory : Contains logs with detailed info about the operation execution. See below . id: String status: String definitionName: String schedulerName: String createdAt: Timestamp input: Any executionHistory: ExecutionHistory","title":"Operation Structure"},{"location":"reference/Operations/#input","text":"Create Scheduler scheduler: Scheduler Create New Scheduler Version scheduler: Scheduler Switch Scheduler Version newActiveVersion: Scheduler Add Rooms amount: Integer Remove Rooms amount: Integer","title":"Input"},{"location":"reference/Operations/#execution-history","text":"createdAt: timestamp event: String createdAt : When did the event happened. event : What happened. E.g. \"Operation failed because...\".","title":"Execution History"},{"location":"reference/Operations/#how-does-maestro-handle-operations","text":"Each scheduler has 1 operation execution (no operations running in parallel for a scheduler). Every operation execution has 1 queue for pending operations. When the worker is ready to work on a new operation, it'll pop from the queue. The operation is executed by the worker following the lifecycle described here .","title":"How does Maestro handle operations"},{"location":"reference/Operations/#state","text":"An operation can have one of the Status below: Pending : When an operation is enqueued to be executed; Evicted : When an operation is unknown or should not be executed By Maestro; In Progress : Operation is currently being executed; Finished : Operation finished; Execution succeeded; Error : Operation finished. Execution failed; Canceled : Operation was canceled by the user.","title":"State"},{"location":"reference/Operations/#state-machine","text":"flowchart TD pending(Pending) in_progress(In Progress) evicted(Evicted) finished(Finished) canceled(Canceled) error(Error) pending --> in_progress; pending --> evicted; in_progress --> finished; in_progress --> error; in_progress --> canceled;","title":"State Machine"},{"location":"reference/Operations/#lifecycle","text":"flowchart TD finish((End)) created(\"Created (Pending)\") evicted(Evicted) error(Error) finished(Finished) canceled(Canceled) should_execute{Should Execute?} execution_succeeded{Success?} err_kind{Error Kind} execute[[Execute]] rollback[[Rollback]] canceled_by_user>Canceled By User] created --> should_execute; should_execute -- No --> evicted --> finish; should_execute -- Yes --> execute; execute --> execution_succeeded; execute --> canceled_by_user --> rollback; execution_succeeded -- Yes --> finished --> finish; execution_succeeded -- No --> rollback; rollback --> err_kind; err_kind -- Canceled --> canceled --> finish; err_kind -- Error --> error --> finish","title":"Lifecycle"},{"location":"reference/Operations/#lease","text":"","title":"Lease"},{"location":"reference/Operations/#what-is-the-operation-lease","text":"Lease is a mechanism to track the operations' execution process and check if we can rely on the current/future operation state.","title":"What is the operation lease"},{"location":"reference/Operations/#why-operations-have-it","text":"Sometimes, an operation might get stuck. It could happen, for example, if the worker crashes during the execution of an operation. To keep track of operations, we assign each operation a Lease. This Lease has a TTL (time to live). When the operation is being executed, this TTL is renewed each time the lease is about to expire while the operation is still in progress. It'll be revoked once the operation is finished.","title":"Why Operations have it"},{"location":"reference/Operations/#troubleshooting","text":"If an operation is fetched and the TTL expired (the TTL is in the past), the operation probably got stuck, and we can't rely upon its current state, nor guarantee the required side effects of the execution or rollback have succeeded. If an operation does not have a Lease, it either did not start at all (should be on the queue) or is already finished. An Active Operation without a Lease is at an invalid state. Maestro do not have a self-healing routine yet for expired operations.","title":"Troubleshooting"},{"location":"reference/Operations/#operation-lease-lifecycle","text":"flowchart TD finish_routine((End)) start_routine((Start)) finish((End)) operation_finished{Op. Finished?} to_execute[Operation to Execute] grant_lease[[Grant Lease]] renew_lease[[Renew Lease]] revoke_lease[[Revoke Lease]] wait_for_ttl(Wait for TTL) execute(Execute Operation) to_execute --> grant_lease; grant_lease --> renew_lease_routine; grant_lease --> execute; subgraph renew_lease_routine [ASYNC Renew Lease Routine] start_routine --> wait_for_ttl; wait_for_ttl --> operation_finished; operation_finished -- Yes --> revoke_lease; operation_finished -- No --> renew_lease --> wait_for_ttl; revoke_lease --> finish_routine; end renew_lease_routine --> finish;","title":"Operation Lease Lifecycle"},{"location":"reference/Operations/#available-operations","text":"For more details on how to use Maestro API, see this section .","title":"Available Operations"},{"location":"reference/Operations/#create-scheduler","text":"Accessed through the POST /schedulers endpoint. Creates the scheduler structure for receiving rooms; The scheduler structure is validated, but the game room is not; If operation fails, rollback feature will delete anything created related to scheduler.","title":"Create Scheduler"},{"location":"reference/Operations/#create-new-scheduler-version","text":"Accessed through the POST /schedulers/:schedulerName endpoint. Creates a validation room (deleted right after). If Maestro cannot receive pings (not forwarded) from validation game room, operation fails; When this operation finishes successfully, it enqueues the \"Switch Active Version\". If operation fails rollback routine deletes anything (except for the operation) created related to new version.","title":"Create New Scheduler Version"},{"location":"reference/Operations/#switch-active-version","text":"Accessed through PUT /schedulers/:schedulerName endpoint. If it's a major change (anything under Scheduler.Spec changed), GRUs are replaced using scheduler maxSurge property; If it's a minor change (Scheduler.Spec haven't changed), GRUs are not replaced;","title":"Switch Active Version"},{"location":"reference/Operations/#add-rooms","text":"Accessed through POST /schedulers/:schedulerName/add-rooms endpoint. If any room fail on creating, the operation fails and created rooms are deleted on rollback feature;","title":"Add Rooms"},{"location":"reference/Operations/#remove-rooms","text":"Accessed through POST /schedulers/:schedulerName/remove-rooms endpoint. Remove rooms based on amount; Rollback routine does nothing.","title":"Remove Rooms"},{"location":"reference/Scheduler/","text":"What is Objectively, a Scheduler is a recipe, and contains all the information for creating game rooms and forwarding rooms information to other services. Also, it's the core entity for operating game rooms in Maestro, since all game rooms are related to a specific scheduler. A game can have multiple schedulers, and each scheduler can have multiple game rooms up and running. flowchart TD etc1(\"...\") etc2(\"...\") etc3(\"...\") subgraph game [Game] subgraph scheduler_1 [Scheduler 1] gameRoom1(\"Game Room (host:port)\") gameRoom2(\"Game Room (host:port)\") etc1 end subgraph scheduler_2 [Scheduler 2] gameRoom3(\"Game Room (host:port)\") gameRoom4(\"Game Room (host:port)\") etc2 end etc3 end How to Operate To directly interact with a Scheduler, the user enqueues operations using the management API. These operations are responsible for creating a scheduler or newer versions, switching an active version, adding/removing rooms, etc. Because of that, everything that happens for a Scheduler can be tracked based on history of the operations executed for that scheduler and the order they were executed. Versions A Scheduler have versions, and each time we want to change scheduler properties, we end-up creating a new version to it. Versions are directly calculated by Maestro, not sent by the client. The client can only switch the active version based on the versions created by Maestro. To switch to an specific version, see this . This version can either be a Minor or a Major change. Major version: Replace the game rooms in a switch active version event. Basically, any change under spec , that are related to the game room directly. Minor version: Don't replace game rooms in a switch active version event. Info such as MaxSurge or forwarders, that do not impact the game rooms. Example A complete Scheduler looks like this: YAML name: scheduler-test game: game-test state: creating portRange: start: 40000 end: 60000 maxSurge: 30% spec: terminationGracePeriod: '100' containers: - name: alpine image: alpine imagePullPolicy: IfNotPresent command: - /bin/sh - '-c' - >- apk add curl && while true; do curl --request POST {{maestro-rooms-api}}/scheduler/$MAESTRO_SCHEDULER_NAME/rooms/$MAESTRO_ROOM_ID/ping --data-raw '{\"status\": \"ready\",\"timestamp\": \"12312312313\"}' && sleep 1; done environment: - name: env-var-name value: env-var-value - name: env-var-field-ref valueFrom: fieldRef: fieldPath: path - name: secret-var-name valueFrom: secretKeyRef: name: secret-name key: secret-key requests: memory: 20Mi cpu: 100m limits: memory: 200Mi cpu: 200m ports: - name: port-name protocol: tcp port: 12345 toleration: maestro affinity: maestro-dedicated forwarders: - name: test enable: true type: gRPC address: {{host}} options: timeout: '1000' metadata: {} autoscaling: enabled: true min: 1 max: 10 policy: type: roomOccupancy parameters: ... // Will vary according to the policy type. JSON { \"name\": \"scheduler-test\", \"game\": \"game-test\", \"portRange\": { \"start\": 40000, \"end\": 60000 }, \"maxSurge\": \"30%\", \"spec\": { \"terminationGracePeriod\": '100', \"containers\": [ { \"name\": \"alpine\", \"image\": \"alpine\", \"imagePullPolicy\": \"IfNotPresent\", \"command\": [ \"/bin/sh\", \"-c\", \"apk add curl && while true; do curl --request POST {{maestro-rooms-api}}/scheduler/$MAESTRO_SCHEDULER_NAME/rooms/$MAESTRO_ROOM_ID/ping --data-raw '{\"status\": \"ready\",\"timestamp\": \"12312312313\"}' && sleep 1; done\" ], \"environment\": [ { \"name\": \"env-var-name\", \"value\": env-var-value }, { \"name\": \"env-var-field-ref\", \"valueFrom\": { \"fieldRef\": { \"fieldPath\": \"path\" } } }, { \"name\": \"secret-var-name\", \"valueFrom\": { \"secretKeyRef\": { \"name\": \"secret-name\", \"key\": \"secret-key\" } } } ], \"requests\": { \"memory\": \"20Mi\", \"cpu\": \"100m\" }, \"limits\": { \"memory\": \"200Mi\", \"cpu\": \"200m\" }, \"ports\": [ { \"name\": \"port-name\", \"protocol\": \"tcp\", \"port\": 12345, } ] } ], \"toleration\": \"maestro\", \"affinity\": \"maestro-dedicated\" }, \"forwarders\": [ { \"name\": \"test\", \"enable\": true, \"type\": \"gRPC\", \"address\": \"{{host}}\", \"options\": { \"timeout\": '1000', \"metadata\": {} } } ] \"autoscaling\": { \"enabled\": true, \"min\": 10, \"max\": 300, \"policy\": { \"type\": \"roomOccupancy\", \"parameters\": { ... // Will vary according to the policy type. } } } } Structure The scheduler is represented as: name: String game: String createdAt: Timestamp maxSurge: String | Integer portRange: PortRange forwarders: Forwarders autoscaling: Autoscaling spec: Spec Name : Scheduler name. This name is unique and will be the same name used for the kubernetes namespace. It's offered by the user in the creation and cannot be changed in the future. It's used as an ID for the scheduler; game : Name of the game which will use the scheduler. The game is important since it's common to use multiple schedulers for a specific game. So you probably will want to fetch all the schedulers from a game; createdAt : Info about the scheduler creation time. Cannot be altered by the user; maxSurge : Value represented in percentage (%) or Integer. Offered by the user on the scheduler creation. Can be altered anytime. Used by Maestro to replace pods. Ex: If maxSurge = 3, the Switch Active Version (Major change) operation will replace pods from 3 to 3; portRange : Range of ports that can be used by Maestro to create GRUs for the specified scheduler. Can be altered by the user anytime. More info here ; forwarders : Maestro can pass ahead info sent by the game rooms, such as Ping (Ready, Occupied, Terminating...), player and rooms events. The receivers can be configured here. More info here ; autoscaling : Optional autoscaling policy configuration. More info here ; spec : Specifications about the game rooms managed by the scheduler, such as containers and environment variables used by them, limits and images. More info here . PortRange The PortRange is used to select a random port for a GRU between start and end . PortRange cannot be null or empty Check if the ports offered are available and can be used . A firewall rule, for example, can affect the connection to the Game Room in the specific port. It is represented as: start: Integer end: Integer Forwarders Forwarders are configured to pass ahead info offered by the game rooms to Maestro. More than one forwarder can be configured for a scheduler. Can be an empty list. It is represented as: - name: String enable: Bool type: String address: String options: timeout: Integer metadata: Object name : Name of the forwarder. Used only for reference (visibility and recognition); enable : Toggle to easily enable/disable the forwarder; type : Type of the forwarder. Right now, only accepts gRPC ; address : Address used by the scheduler to forward events. E.g. 'api.example.com:8080'; options : Optional parameters. timeout : Timeout value for an event to successfully be forwarded; metadata : Object that can contain any useful information for the game team. Will be forwarded with the events from Maestro. Spec Contains vital information about the game rooms. Be aware that the spec is the most related aspect of the scheduler interacting with the runtime. It might be important to understand the basics of kubernetes before deep diving into the Maestro scheduler itself. Cannot be empty or null. It is represented as: terminationGracePeriod: Integer containers: Containers toleration: String affinity: String terminationGracePeriod : Required integer value. Must be greater than 0. When a game room receives the signal to be deleted, it will take this value (in milliseconds) to be completely deleted; containers : Contain the information about the game room, such as the image and environment variables. This is a list since the game room can be compounded by more than two containers; toleration : Kubernetes specific. Represents the toleration value for all GRUs on the scheduler. See more ; affinity : Kubernetes specific. Represents the affinity value for all GRUs on the scheduler. See more . Containers Contain the information about the game room, such as the image and environment variables. Cannot be an empty list. It is represented as: - name: String image: String imagePullPolicy: String command: Array<String> environment: EnvironemntVariables requests: memory: String cpu: String limits: memory: String cpu: String ports: Ports name : Name of the container, used only for reference and can be changed by the user anytime. image : Docker image to be used for the container. Represented as a link. imagePullPolicy : Kubernetes specific. See here for reference. command : List of commands that should be executed by the image on execution. E.g. here . environment : List of environment variables. See here . requests and limits : Kubernetes specific. See here . ports : The list of ports your game server exposes. See here . Environment Variables List of environment variables used by the GRU. Can be an empty list. There are, now, 3 supported formats by Maestro: Simple name-value format. With Name/Value - name: String value: String Exposing pod fields (kubernetes specific). See here . With FieldRef/FieldPath - name: String valueFrom: fieldRef: fieldPath: String Secrets as environment variables (kubernetes specific). See here . With Secret - name: String valueFrom: secretKeyRef: name: String key: String Ports The list of ports your game server exposes. Can be an empty list. It is represented as: - name: String protocol: String port: Integer name : Name of the port. Facilitates on recognition; protocol : Port protocol. Can be UDP, TCP or SCTP.; port : The port exposed.","title":"Scheduler"},{"location":"reference/Scheduler/#what-is","text":"Objectively, a Scheduler is a recipe, and contains all the information for creating game rooms and forwarding rooms information to other services. Also, it's the core entity for operating game rooms in Maestro, since all game rooms are related to a specific scheduler. A game can have multiple schedulers, and each scheduler can have multiple game rooms up and running. flowchart TD etc1(\"...\") etc2(\"...\") etc3(\"...\") subgraph game [Game] subgraph scheduler_1 [Scheduler 1] gameRoom1(\"Game Room (host:port)\") gameRoom2(\"Game Room (host:port)\") etc1 end subgraph scheduler_2 [Scheduler 2] gameRoom3(\"Game Room (host:port)\") gameRoom4(\"Game Room (host:port)\") etc2 end etc3 end","title":"What is"},{"location":"reference/Scheduler/#how-to-operate","text":"To directly interact with a Scheduler, the user enqueues operations using the management API. These operations are responsible for creating a scheduler or newer versions, switching an active version, adding/removing rooms, etc. Because of that, everything that happens for a Scheduler can be tracked based on history of the operations executed for that scheduler and the order they were executed.","title":"How to Operate"},{"location":"reference/Scheduler/#versions","text":"A Scheduler have versions, and each time we want to change scheduler properties, we end-up creating a new version to it. Versions are directly calculated by Maestro, not sent by the client. The client can only switch the active version based on the versions created by Maestro. To switch to an specific version, see this . This version can either be a Minor or a Major change. Major version: Replace the game rooms in a switch active version event. Basically, any change under spec , that are related to the game room directly. Minor version: Don't replace game rooms in a switch active version event. Info such as MaxSurge or forwarders, that do not impact the game rooms.","title":"Versions"},{"location":"reference/Scheduler/#example","text":"A complete Scheduler looks like this: YAML name: scheduler-test game: game-test state: creating portRange: start: 40000 end: 60000 maxSurge: 30% spec: terminationGracePeriod: '100' containers: - name: alpine image: alpine imagePullPolicy: IfNotPresent command: - /bin/sh - '-c' - >- apk add curl && while true; do curl --request POST {{maestro-rooms-api}}/scheduler/$MAESTRO_SCHEDULER_NAME/rooms/$MAESTRO_ROOM_ID/ping --data-raw '{\"status\": \"ready\",\"timestamp\": \"12312312313\"}' && sleep 1; done environment: - name: env-var-name value: env-var-value - name: env-var-field-ref valueFrom: fieldRef: fieldPath: path - name: secret-var-name valueFrom: secretKeyRef: name: secret-name key: secret-key requests: memory: 20Mi cpu: 100m limits: memory: 200Mi cpu: 200m ports: - name: port-name protocol: tcp port: 12345 toleration: maestro affinity: maestro-dedicated forwarders: - name: test enable: true type: gRPC address: {{host}} options: timeout: '1000' metadata: {} autoscaling: enabled: true min: 1 max: 10 policy: type: roomOccupancy parameters: ... // Will vary according to the policy type. JSON { \"name\": \"scheduler-test\", \"game\": \"game-test\", \"portRange\": { \"start\": 40000, \"end\": 60000 }, \"maxSurge\": \"30%\", \"spec\": { \"terminationGracePeriod\": '100', \"containers\": [ { \"name\": \"alpine\", \"image\": \"alpine\", \"imagePullPolicy\": \"IfNotPresent\", \"command\": [ \"/bin/sh\", \"-c\", \"apk add curl && while true; do curl --request POST {{maestro-rooms-api}}/scheduler/$MAESTRO_SCHEDULER_NAME/rooms/$MAESTRO_ROOM_ID/ping --data-raw '{\"status\": \"ready\",\"timestamp\": \"12312312313\"}' && sleep 1; done\" ], \"environment\": [ { \"name\": \"env-var-name\", \"value\": env-var-value }, { \"name\": \"env-var-field-ref\", \"valueFrom\": { \"fieldRef\": { \"fieldPath\": \"path\" } } }, { \"name\": \"secret-var-name\", \"valueFrom\": { \"secretKeyRef\": { \"name\": \"secret-name\", \"key\": \"secret-key\" } } } ], \"requests\": { \"memory\": \"20Mi\", \"cpu\": \"100m\" }, \"limits\": { \"memory\": \"200Mi\", \"cpu\": \"200m\" }, \"ports\": [ { \"name\": \"port-name\", \"protocol\": \"tcp\", \"port\": 12345, } ] } ], \"toleration\": \"maestro\", \"affinity\": \"maestro-dedicated\" }, \"forwarders\": [ { \"name\": \"test\", \"enable\": true, \"type\": \"gRPC\", \"address\": \"{{host}}\", \"options\": { \"timeout\": '1000', \"metadata\": {} } } ] \"autoscaling\": { \"enabled\": true, \"min\": 10, \"max\": 300, \"policy\": { \"type\": \"roomOccupancy\", \"parameters\": { ... // Will vary according to the policy type. } } } }","title":"Example"},{"location":"reference/Scheduler/#structure","text":"The scheduler is represented as: name: String game: String createdAt: Timestamp maxSurge: String | Integer portRange: PortRange forwarders: Forwarders autoscaling: Autoscaling spec: Spec Name : Scheduler name. This name is unique and will be the same name used for the kubernetes namespace. It's offered by the user in the creation and cannot be changed in the future. It's used as an ID for the scheduler; game : Name of the game which will use the scheduler. The game is important since it's common to use multiple schedulers for a specific game. So you probably will want to fetch all the schedulers from a game; createdAt : Info about the scheduler creation time. Cannot be altered by the user; maxSurge : Value represented in percentage (%) or Integer. Offered by the user on the scheduler creation. Can be altered anytime. Used by Maestro to replace pods. Ex: If maxSurge = 3, the Switch Active Version (Major change) operation will replace pods from 3 to 3; portRange : Range of ports that can be used by Maestro to create GRUs for the specified scheduler. Can be altered by the user anytime. More info here ; forwarders : Maestro can pass ahead info sent by the game rooms, such as Ping (Ready, Occupied, Terminating...), player and rooms events. The receivers can be configured here. More info here ; autoscaling : Optional autoscaling policy configuration. More info here ; spec : Specifications about the game rooms managed by the scheduler, such as containers and environment variables used by them, limits and images. More info here .","title":"Structure"},{"location":"reference/Scheduler/#portrange","text":"The PortRange is used to select a random port for a GRU between start and end . PortRange cannot be null or empty Check if the ports offered are available and can be used . A firewall rule, for example, can affect the connection to the Game Room in the specific port. It is represented as: start: Integer end: Integer","title":"PortRange"},{"location":"reference/Scheduler/#forwarders","text":"Forwarders are configured to pass ahead info offered by the game rooms to Maestro. More than one forwarder can be configured for a scheduler. Can be an empty list. It is represented as: - name: String enable: Bool type: String address: String options: timeout: Integer metadata: Object name : Name of the forwarder. Used only for reference (visibility and recognition); enable : Toggle to easily enable/disable the forwarder; type : Type of the forwarder. Right now, only accepts gRPC ; address : Address used by the scheduler to forward events. E.g. 'api.example.com:8080'; options : Optional parameters. timeout : Timeout value for an event to successfully be forwarded; metadata : Object that can contain any useful information for the game team. Will be forwarded with the events from Maestro.","title":"Forwarders"},{"location":"reference/Scheduler/#spec","text":"Contains vital information about the game rooms. Be aware that the spec is the most related aspect of the scheduler interacting with the runtime. It might be important to understand the basics of kubernetes before deep diving into the Maestro scheduler itself. Cannot be empty or null. It is represented as: terminationGracePeriod: Integer containers: Containers toleration: String affinity: String terminationGracePeriod : Required integer value. Must be greater than 0. When a game room receives the signal to be deleted, it will take this value (in milliseconds) to be completely deleted; containers : Contain the information about the game room, such as the image and environment variables. This is a list since the game room can be compounded by more than two containers; toleration : Kubernetes specific. Represents the toleration value for all GRUs on the scheduler. See more ; affinity : Kubernetes specific. Represents the affinity value for all GRUs on the scheduler. See more .","title":"Spec"},{"location":"reference/Scheduler/#containers","text":"Contain the information about the game room, such as the image and environment variables. Cannot be an empty list. It is represented as: - name: String image: String imagePullPolicy: String command: Array<String> environment: EnvironemntVariables requests: memory: String cpu: String limits: memory: String cpu: String ports: Ports name : Name of the container, used only for reference and can be changed by the user anytime. image : Docker image to be used for the container. Represented as a link. imagePullPolicy : Kubernetes specific. See here for reference. command : List of commands that should be executed by the image on execution. E.g. here . environment : List of environment variables. See here . requests and limits : Kubernetes specific. See here . ports : The list of ports your game server exposes. See here .","title":"Containers"},{"location":"reference/Scheduler/#environment-variables","text":"List of environment variables used by the GRU. Can be an empty list. There are, now, 3 supported formats by Maestro: Simple name-value format. With Name/Value - name: String value: String Exposing pod fields (kubernetes specific). See here . With FieldRef/FieldPath - name: String valueFrom: fieldRef: fieldPath: String Secrets as environment variables (kubernetes specific). See here . With Secret - name: String valueFrom: secretKeyRef: name: String key: String","title":"Environment Variables"},{"location":"reference/Scheduler/#ports","text":"The list of ports your game server exposes. Can be an empty list. It is represented as: - name: String protocol: String port: Integer name : Name of the port. Facilitates on recognition; protocol : Port protocol. Can be UDP, TCP or SCTP.; port : The port exposed.","title":"Ports"},{"location":"tutorials/Autoscaling/","text":"Configuring Scheduler Autoscaling Prerequisites Have a game room container image that communicates with maestro through Maestro's rooms API Learning Outcomes After finishing this tutorial you will understand how: to configure autoscaling policies for your scheduler What is Autoscaling is an optional feature in which the user can choose and parametrize different autoscaling policies that maestro will use to automatically scale the number of rooms in the scheduler. Maestro has an internal process that periodically keeps checking if it needs to create or delete game rooms for the given scheduler, if autoscaling is not configured or enabled, it will always try to maintain the current number of rooms equal to roomsReplicas scheduler property. If autoscaling is configured and enabled, it will use the configured autoscaling policy to decide if it needs to scale up (create more rooms), scale down (delete rooms) or do nothing . flowchart TD finish((End)) add_rooms_operation(Enqueue add rooms operation) remove_rooms_operation(Enqueue remove rooms) use_rooms_replicas(Use rooms replicas to calculate the desired number of rooms) autoscaling_enabled{Autoscaling configured and enabled?} decide_operation{Compare current number of rooms with the desired amount.} use_autoscaling[Use Autoscaling policy to calculate the desired number of rooms coerced in min-max range] autoscaling_enabled -- No --> use_rooms_replicas; autoscaling_enabled -- Yes --> use_autoscaling; use_autoscaling --> decide_operation; use_rooms_replicas --> decide_operation; decide_operation -- desired > actual --> add_rooms_operation --> finish; decide_operation -- desired == actual --> finish; decide_operation -- desired < actual --> remove_rooms_operation --> finish; Currently, the sync interval is configured by environment variable MAESTRO_WORKERS_OPERATIONEXECUTION_HEALTHCONTROLLERINTERVAL . By default, the scheduler does not have autoscaling configured. How to configure and enable autoscaling To get autoscaling working in your scheduler, firstly you need to configure an autoscaling policy and enable it, this autoscaling configuration resides in the root of the scheduler structure itself. YAML version name: String game: String ... autoscaling: enabled: true min: 1 max: 10 policy: type: roomOccupancy parameters: ... // Will vary according to the policy type. JSON version { \"name\": \"test\", \"game\": \"multiplayer\", ... \"autoscaling\": { \"enabled\": true, \"min\": 10, \"max\": 300, \"policy\": { \"type\": \"roomOccupancy\", \"parameters\": { ... // Will vary according to the policy type. } } } } enabled [boolean]: A value that can be true or false, indicating if the autoscaling feature is enabled/disabled for the given scheduler. Default: false. min [integer]: Minimum number of rooms the scheduler should have, it must be greater than zero. For zero value, disable autoscaling and set \"roomsReplicas\" to 0. max [integer]: Maximum number of rooms the scheduler can have. It must be greater than min, or can be -1 (to have no limit). policy [struct] : This field holds information regarding the autoscaling policy that will be used if the autoscaling feature is enabled: type [string]: Define the policy type that will be used, must be one of the policy types maestro provides . parameters [struct]: This field will contain arbitrary fields that will vary according to the chosen policy type . Policy Types Maestro has a set of predefined policy types that can be used to configure the autoscaling, each policy will implement a specific strategy for calculating the desired number of rooms and will have its configurable parameters. Room Occupancy Policy The basic concept of this policy is to scale the scheduler up or down based on the actual room occupancy rate, by defining a \"buffer\" percentage of ready rooms that Maestro must keep. The desired number of rooms will be given by the following formula: desiredNumberOfRooms = \u2308(numberOfOccupiedRooms/ (1- readyTarget) )\u2309 So basically Maestro will constantly try to maintain a certain percentage of rooms in ready state, by looking at the actual room occupancy rate (number of rooms in occupied state). Room Occupancy Policy Parameters readyTarget [float]: The percentage (in decimal value) of rooms that Maestro should try to keep in ready state, must be a value between 0.1 and 0.9. Example YAML version name: String game: String ... autoscaling: enabled: true min: 1 max: 10 policy: type: roomOccupancy parameters: roomOccupancy: readyTarget: 0.5 JSON version { \"autoscaling\": { \"enabled\": true, \"min\": 10, \"max\": 300, \"policy\": { \"type\": \"roomOccupancy\", \"parameters\": { \"roomOccupancy\": { \"readyTarget\": 0.5 } } } } } Below are some simulated examples of how the room occupancy policy will behave: Note that the autoscaling decision will always be limited by the min-max values! . totalRooms occupiedRooms readyTarget desiredNumberOfRooms autoscalingDecision 100 80 0.5 160 Scale Up: +60 100 50 0.5 100 Do Nothing: 0 100 30 0.5 60 Scale Down: -40 50 40 0.3 58 Scale Up: +8 50 35 0.3 50 Do Nothing: 0 50 10 0.3 15 Scale Down: -35 10 5 0.9 50 Scale Up: +40 10 1 0.9 10 Do Nothing: 0 10 1 0.8 5 Scale Down: -5 5 5 0.1 6 Scale Up: +1 1 1 0.3 2 Scale Up: +1 2 2 0.9 20 Scale Up: +18","title":"Autoscaling"},{"location":"tutorials/Autoscaling/#configuring-scheduler-autoscaling","text":"","title":"Configuring Scheduler Autoscaling"},{"location":"tutorials/Autoscaling/#prerequisites","text":"Have a game room container image that communicates with maestro through Maestro's rooms API","title":"Prerequisites"},{"location":"tutorials/Autoscaling/#learning-outcomes","text":"After finishing this tutorial you will understand how: to configure autoscaling policies for your scheduler","title":"Learning Outcomes"},{"location":"tutorials/Autoscaling/#what-is","text":"Autoscaling is an optional feature in which the user can choose and parametrize different autoscaling policies that maestro will use to automatically scale the number of rooms in the scheduler. Maestro has an internal process that periodically keeps checking if it needs to create or delete game rooms for the given scheduler, if autoscaling is not configured or enabled, it will always try to maintain the current number of rooms equal to roomsReplicas scheduler property. If autoscaling is configured and enabled, it will use the configured autoscaling policy to decide if it needs to scale up (create more rooms), scale down (delete rooms) or do nothing . flowchart TD finish((End)) add_rooms_operation(Enqueue add rooms operation) remove_rooms_operation(Enqueue remove rooms) use_rooms_replicas(Use rooms replicas to calculate the desired number of rooms) autoscaling_enabled{Autoscaling configured and enabled?} decide_operation{Compare current number of rooms with the desired amount.} use_autoscaling[Use Autoscaling policy to calculate the desired number of rooms coerced in min-max range] autoscaling_enabled -- No --> use_rooms_replicas; autoscaling_enabled -- Yes --> use_autoscaling; use_autoscaling --> decide_operation; use_rooms_replicas --> decide_operation; decide_operation -- desired > actual --> add_rooms_operation --> finish; decide_operation -- desired == actual --> finish; decide_operation -- desired < actual --> remove_rooms_operation --> finish; Currently, the sync interval is configured by environment variable MAESTRO_WORKERS_OPERATIONEXECUTION_HEALTHCONTROLLERINTERVAL . By default, the scheduler does not have autoscaling configured.","title":"What is"},{"location":"tutorials/Autoscaling/#how-to-configure-and-enable-autoscaling","text":"To get autoscaling working in your scheduler, firstly you need to configure an autoscaling policy and enable it, this autoscaling configuration resides in the root of the scheduler structure itself. YAML version name: String game: String ... autoscaling: enabled: true min: 1 max: 10 policy: type: roomOccupancy parameters: ... // Will vary according to the policy type. JSON version { \"name\": \"test\", \"game\": \"multiplayer\", ... \"autoscaling\": { \"enabled\": true, \"min\": 10, \"max\": 300, \"policy\": { \"type\": \"roomOccupancy\", \"parameters\": { ... // Will vary according to the policy type. } } } } enabled [boolean]: A value that can be true or false, indicating if the autoscaling feature is enabled/disabled for the given scheduler. Default: false. min [integer]: Minimum number of rooms the scheduler should have, it must be greater than zero. For zero value, disable autoscaling and set \"roomsReplicas\" to 0. max [integer]: Maximum number of rooms the scheduler can have. It must be greater than min, or can be -1 (to have no limit). policy [struct] : This field holds information regarding the autoscaling policy that will be used if the autoscaling feature is enabled: type [string]: Define the policy type that will be used, must be one of the policy types maestro provides . parameters [struct]: This field will contain arbitrary fields that will vary according to the chosen policy type .","title":"How to configure and enable autoscaling"},{"location":"tutorials/Autoscaling/#policy-types","text":"Maestro has a set of predefined policy types that can be used to configure the autoscaling, each policy will implement a specific strategy for calculating the desired number of rooms and will have its configurable parameters.","title":"Policy Types"},{"location":"tutorials/Autoscaling/#room-occupancy-policy","text":"The basic concept of this policy is to scale the scheduler up or down based on the actual room occupancy rate, by defining a \"buffer\" percentage of ready rooms that Maestro must keep. The desired number of rooms will be given by the following formula: desiredNumberOfRooms = \u2308(numberOfOccupiedRooms/ (1- readyTarget) )\u2309 So basically Maestro will constantly try to maintain a certain percentage of rooms in ready state, by looking at the actual room occupancy rate (number of rooms in occupied state).","title":"Room Occupancy Policy"},{"location":"tutorials/Autoscaling/#room-occupancy-policy-parameters","text":"readyTarget [float]: The percentage (in decimal value) of rooms that Maestro should try to keep in ready state, must be a value between 0.1 and 0.9.","title":"Room Occupancy Policy Parameters"},{"location":"tutorials/Autoscaling/#example","text":"YAML version name: String game: String ... autoscaling: enabled: true min: 1 max: 10 policy: type: roomOccupancy parameters: roomOccupancy: readyTarget: 0.5 JSON version { \"autoscaling\": { \"enabled\": true, \"min\": 10, \"max\": 300, \"policy\": { \"type\": \"roomOccupancy\", \"parameters\": { \"roomOccupancy\": { \"readyTarget\": 0.5 } } } } } Below are some simulated examples of how the room occupancy policy will behave: Note that the autoscaling decision will always be limited by the min-max values! . totalRooms occupiedRooms readyTarget desiredNumberOfRooms autoscalingDecision 100 80 0.5 160 Scale Up: +60 100 50 0.5 100 Do Nothing: 0 100 30 0.5 60 Scale Down: -40 50 40 0.3 58 Scale Up: +8 50 35 0.3 50 Do Nothing: 0 50 10 0.3 15 Scale Down: -35 10 5 0.9 50 Scale Up: +40 10 1 0.9 10 Do Nothing: 0 10 1 0.8 5 Scale Down: -5 5 5 0.1 6 Scale Up: +1 1 1 0.3 2 Scale Up: +1 2 2 0.9 20 Scale Up: +18","title":"Example"},{"location":"tutorials/Development/","text":"Development Setting up the environment Grpc gateway In order to run make generate with success, you need to have grpc-gateway dependencies installed with the following command: go install \\ github.com/grpc-ecosystem/grpc-gateway/v2/protoc-gen-grpc-gateway \\ github.com/grpc-ecosystem/grpc-gateway/v2/protoc-gen-openapiv2 \\ google.golang.org/protobuf/cmd/protoc-gen-go \\ google.golang.org/grpc/cmd/protoc-gen-go-grpc Golang version The project requires golang version 1.18 or higher. Building and running Run make setup to get all required modules Run make generate to generate mocks, protos and wire (dependency injection) Run make deps/up to startup service dependencies Run make migrate to migrate database with the most updated schema Running tests Run make run/unit-tests to run all unit tests Run make run/integration-tests to run all integration tests Run make run/e2e-tests to run all E2E tests. NOTE: Currently it is not possible to run it with the development environment set. This command will stop the dev dependencies before running. Run make lint to run all registered linters Running locally To help you get along with Maestro, by the end of this section you should have a scheduler up and running. Prerequisites Golang v1.18+ Linux/MacOS environment Docker Clone Repository Clone the repository to your favorite folder. Getting Maestro up and running For this step, you need docker running on your machine. WARNING: Ensure using cgroupv1 K3s needs to use the deprecated cgroupv1 , to successfully run the project in your machine ensure that your current docker use this version. In the folder where the project was cloned, simply run: make maestro/start This will build and start all containers needed by Maestro, such as databases and maestro-modules. This will also start all maestro components, including rooms api, management api, runtime watcher, and execution worker. Because of that, be aware that it might take some time to finish. Find rooms-api address To simulate a game room, it's important to find the address of running rooms-api on the local network. To do that, with Maestro containers running, simply use: docker inspect -f '{{range.NetworkSettings.Networks}}{{.Gateway}}{{end}}' {{ROOMS_API_CONTAINER_NAME}} This command should give you an IP address. This IP is important because the game rooms will use it to communicate their status. Create a scheduler If everything is working as expected now, each Maestro-module is up and running. Use the command below to create a new scheduler: Be aware to change the {{ROOMS_API_ADDRESS}} for the one found above. curl --request POST \\ --url http://localhost:8080/schedulers \\ --header 'Content-Type: application/json' \\ --data '{ \"name\": \"scheduler-run-local\", \"game\": \"game-test\", \"state\": \"creating\", \"portRange\": { \"start\": 1, \"end\": 1000 }, \"maxSurge\": \"10%\", \"spec\": { \"terminationGracePeriod\": \"100\", \"containers\": [ { \"name\": \"alpine\", \"image\": \"alpine\", \"imagePullPolicy\": \"IfNotPresent\", \"command\": [ \"sh\", \"-c\", \"apk add curl && while true; do curl --request PUT {{ROOMS_API_ADDRESS}}:8070/scheduler/$MAESTRO_SCHEDULER_NAME/rooms/$MAESTRO_ROOM_ID/ping --data-raw '\\''{\\\"status\\\": \\\"ready\\\",\\\"timestamp\\\": \\\"12312312313\\\"}'\\'' && sleep 5; done\" ], \"environment\": [], \"requests\": { \"memory\": \"100Mi\", \"cpu\": \"100m\" }, \"limits\": { \"memory\": \"200Mi\", \"cpu\": \"200m\" }, \"ports\": [ { \"name\": \"port-name\", \"protocol\": \"tcp\", \"port\": 12345 } ] } ], \"toleration\": \"\", \"affinity\": \"\" }, \"forwarders\": [] }' Congratulations If you followed the steps above you have Maestro running in your local machine, and with a scheduler to try different operations on it. Feel free to explore the available endpoints in the API hitting directly the management-API. If you have any doubts or feedbacks regarding this process, feel free to reach out in Maestro's GitHub repository and open an issue/question.","title":"Developers"},{"location":"tutorials/Development/#development","text":"","title":"Development"},{"location":"tutorials/Development/#setting-up-the-environment","text":"","title":"Setting up the environment"},{"location":"tutorials/Development/#grpc-gateway","text":"In order to run make generate with success, you need to have grpc-gateway dependencies installed with the following command: go install \\ github.com/grpc-ecosystem/grpc-gateway/v2/protoc-gen-grpc-gateway \\ github.com/grpc-ecosystem/grpc-gateway/v2/protoc-gen-openapiv2 \\ google.golang.org/protobuf/cmd/protoc-gen-go \\ google.golang.org/grpc/cmd/protoc-gen-go-grpc","title":"Grpc gateway"},{"location":"tutorials/Development/#golang-version","text":"The project requires golang version 1.18 or higher.","title":"Golang version"},{"location":"tutorials/Development/#building-and-running","text":"Run make setup to get all required modules Run make generate to generate mocks, protos and wire (dependency injection) Run make deps/up to startup service dependencies Run make migrate to migrate database with the most updated schema","title":"Building and running"},{"location":"tutorials/Development/#running-tests","text":"Run make run/unit-tests to run all unit tests Run make run/integration-tests to run all integration tests Run make run/e2e-tests to run all E2E tests. NOTE: Currently it is not possible to run it with the development environment set. This command will stop the dev dependencies before running. Run make lint to run all registered linters","title":"Running tests"},{"location":"tutorials/Development/#running-locally","text":"To help you get along with Maestro, by the end of this section you should have a scheduler up and running.","title":"Running locally"},{"location":"tutorials/Development/#prerequisites","text":"Golang v1.18+ Linux/MacOS environment Docker","title":"Prerequisites"},{"location":"tutorials/Development/#clone-repository","text":"Clone the repository to your favorite folder.","title":"Clone Repository"},{"location":"tutorials/Development/#getting-maestro-up-and-running","text":"For this step, you need docker running on your machine. WARNING: Ensure using cgroupv1 K3s needs to use the deprecated cgroupv1 , to successfully run the project in your machine ensure that your current docker use this version. In the folder where the project was cloned, simply run: make maestro/start This will build and start all containers needed by Maestro, such as databases and maestro-modules. This will also start all maestro components, including rooms api, management api, runtime watcher, and execution worker. Because of that, be aware that it might take some time to finish.","title":"Getting Maestro up and running"},{"location":"tutorials/Development/#find-rooms-api-address","text":"To simulate a game room, it's important to find the address of running rooms-api on the local network. To do that, with Maestro containers running, simply use: docker inspect -f '{{range.NetworkSettings.Networks}}{{.Gateway}}{{end}}' {{ROOMS_API_CONTAINER_NAME}} This command should give you an IP address. This IP is important because the game rooms will use it to communicate their status.","title":"Find rooms-api address"},{"location":"tutorials/Development/#create-a-scheduler","text":"If everything is working as expected now, each Maestro-module is up and running. Use the command below to create a new scheduler: Be aware to change the {{ROOMS_API_ADDRESS}} for the one found above. curl --request POST \\ --url http://localhost:8080/schedulers \\ --header 'Content-Type: application/json' \\ --data '{ \"name\": \"scheduler-run-local\", \"game\": \"game-test\", \"state\": \"creating\", \"portRange\": { \"start\": 1, \"end\": 1000 }, \"maxSurge\": \"10%\", \"spec\": { \"terminationGracePeriod\": \"100\", \"containers\": [ { \"name\": \"alpine\", \"image\": \"alpine\", \"imagePullPolicy\": \"IfNotPresent\", \"command\": [ \"sh\", \"-c\", \"apk add curl && while true; do curl --request PUT {{ROOMS_API_ADDRESS}}:8070/scheduler/$MAESTRO_SCHEDULER_NAME/rooms/$MAESTRO_ROOM_ID/ping --data-raw '\\''{\\\"status\\\": \\\"ready\\\",\\\"timestamp\\\": \\\"12312312313\\\"}'\\'' && sleep 5; done\" ], \"environment\": [], \"requests\": { \"memory\": \"100Mi\", \"cpu\": \"100m\" }, \"limits\": { \"memory\": \"200Mi\", \"cpu\": \"200m\" }, \"ports\": [ { \"name\": \"port-name\", \"protocol\": \"tcp\", \"port\": 12345 } ] } ], \"toleration\": \"\", \"affinity\": \"\" }, \"forwarders\": [] }'","title":"Create a scheduler"},{"location":"tutorials/Development/#congratulations","text":"If you followed the steps above you have Maestro running in your local machine, and with a scheduler to try different operations on it. Feel free to explore the available endpoints in the API hitting directly the management-API. If you have any doubts or feedbacks regarding this process, feel free to reach out in Maestro's GitHub repository and open an issue/question.","title":"Congratulations"},{"location":"tutorials/EventsForwarding/","text":"Configuring Events Forwarding Prerequisites Have a game room container image that communicates with maestro through Maestro's rooms API Learning Outcomes After finishing this tutorial you will understand how: to configure your room (ping) and player events to be forwarded to an external service (e.g. a matchmaking service) What is Events forwarding is an optional feature in which every room event or player event is forwarded to an external service. Through rooms API, Maestro provides several endpoints for receiving events from the game rooms. These events can be either room events (like room changing state from ready to occupied) or player events (like player joining or leaving the room). Maestro rely only on room events for managing the game rooms, player events endpoint is designed to be used exclusively with the events forwarding feature, since maestro does not depend on this information. Usually Maestro is used with a Matchmaking service, and a matchmaking service generally will need to keep up-to-date with the pool of game rooms that are available or not. Events forwarding feature exists for facilitating this integration, even being possible to make game rooms communicate with matchmaker directly. How to configure and enable events forwarder To get events forwarding working in your scheduler, firstly you need to configure the events forwarder and enable it, this forwarder configuration resides in the root of the scheduler structure itself. YAML version name: String game: String ... forwarders: - name: matchmaking enable: true type: gRPC address: 'external-matchmaker.svc.cluster.local:80' options: timeout: '1000' metadata: ... // Will vary according to the policy type. JSON version { \"name\": \"String\", \"game\": \"String\", ... \"forwarders\": [ { \"name\": \"matchmaking\", \"enable\": true, \"type\": \"gRPC\", \"address\": \"external-matchmaker.svc.cluster.local:80\", \"options\": { \"timeout\": \"1000\", \"metadata\": { ... // Will vary according to the user needs. } } } ] } name : Name of the forwarder. Used only for reference (visibility and recognition); enable : Toggle to easily enable/disable the forwarder; type : Type of the forwarder. Right now, only accepts gRPC ; address : Address used by the scheduler to forward events. E.g. 'api.example.com:8080'; options : Optional parameters. timeout : Timeout value for an event to successfully be forwarded; metadata : Arbitrary metadata object that can contain any data that will be embedded in all event that is forwarded. Events Forwarding Types Currently, Maestro only supports gRPC forwarder type. GRPC This event forwarding type uses the GRPCForwarder service proto definition to forward events, this means that the external service should use gRPC protocol and implement this service to receive events.","title":"Events Forwarding"},{"location":"tutorials/EventsForwarding/#configuring-events-forwarding","text":"","title":"Configuring Events Forwarding"},{"location":"tutorials/EventsForwarding/#prerequisites","text":"Have a game room container image that communicates with maestro through Maestro's rooms API","title":"Prerequisites"},{"location":"tutorials/EventsForwarding/#learning-outcomes","text":"After finishing this tutorial you will understand how: to configure your room (ping) and player events to be forwarded to an external service (e.g. a matchmaking service)","title":"Learning Outcomes"},{"location":"tutorials/EventsForwarding/#what-is","text":"Events forwarding is an optional feature in which every room event or player event is forwarded to an external service. Through rooms API, Maestro provides several endpoints for receiving events from the game rooms. These events can be either room events (like room changing state from ready to occupied) or player events (like player joining or leaving the room). Maestro rely only on room events for managing the game rooms, player events endpoint is designed to be used exclusively with the events forwarding feature, since maestro does not depend on this information. Usually Maestro is used with a Matchmaking service, and a matchmaking service generally will need to keep up-to-date with the pool of game rooms that are available or not. Events forwarding feature exists for facilitating this integration, even being possible to make game rooms communicate with matchmaker directly.","title":"What is"},{"location":"tutorials/EventsForwarding/#how-to-configure-and-enable-events-forwarder","text":"To get events forwarding working in your scheduler, firstly you need to configure the events forwarder and enable it, this forwarder configuration resides in the root of the scheduler structure itself. YAML version name: String game: String ... forwarders: - name: matchmaking enable: true type: gRPC address: 'external-matchmaker.svc.cluster.local:80' options: timeout: '1000' metadata: ... // Will vary according to the policy type. JSON version { \"name\": \"String\", \"game\": \"String\", ... \"forwarders\": [ { \"name\": \"matchmaking\", \"enable\": true, \"type\": \"gRPC\", \"address\": \"external-matchmaker.svc.cluster.local:80\", \"options\": { \"timeout\": \"1000\", \"metadata\": { ... // Will vary according to the user needs. } } } ] } name : Name of the forwarder. Used only for reference (visibility and recognition); enable : Toggle to easily enable/disable the forwarder; type : Type of the forwarder. Right now, only accepts gRPC ; address : Address used by the scheduler to forward events. E.g. 'api.example.com:8080'; options : Optional parameters. timeout : Timeout value for an event to successfully be forwarded; metadata : Arbitrary metadata object that can contain any data that will be embedded in all event that is forwarded.","title":"How to configure and enable events forwarder"},{"location":"tutorials/EventsForwarding/#events-forwarding-types","text":"Currently, Maestro only supports gRPC forwarder type.","title":"Events Forwarding Types"},{"location":"tutorials/EventsForwarding/#grpc","text":"This event forwarding type uses the GRPCForwarder service proto definition to forward events, this means that the external service should use gRPC protocol and implement this service to receive events.","title":"GRPC"},{"location":"tutorials/GettingStarted/","text":"Getting Started Guide Prerequisites Have a game room container image Learning Outcomes After finishing this tutorial you will understand how: to set up your game room to communicate its health status with maestro to configure a new scheduler in maestro with a fixed number of replicas Configuring your game room For Maestro to be able to manage your game rooms, you need to ensure that your game room sends a periodic heartbeat to maestro. This heartbeat is what we call ping , in which the room is able to inform its status (such as ready or occupied ) to maestro. For this, you can use maestro-client sdk, if you are using unity, or you can call Maestro rooms API directly using two env vars that are configured in every game room managed by maestro by default. PUT scheduler/$MAESTRO_SCHEDULER_NAME/rooms/$MAESTRO_ROOM_ID/ping { \"status\": \"ready\", \"timestamp\": \"12312312313\" } The status field can be: - ready : the room is ready to accept players. - occupied : the room is occupied by one or more matches, and is not ready to accept more players. - terminating : the room is terminating, and will not accept any new players. Create a scheduler Use the command below to create a new scheduler, this will make a POST request for /schedulers endpoint. You need to change some parameters according to your game room image needs, for further details on all scheduler fields check the reference : image : your game room image. game : your game name (a same game can have multiple schedulers). name : your scheduler name (usually, the stack name). spec.command : any command that is required to run your game room. spec.environment : any environment variable required to run your game room. spec.ports : any port that must be exposed for clients to connect to the game room curl --request POST \\ --url https://<maestro-url>/schedulers \\ --header 'Content-Type: application/json' \\ --header \"Authorization: Basic <user:pass in base64>\" \\ --data '{ \"name\": \"<your-scheduler-name-here>\", \"game\": \"<your-game-name-here>\", \"roomsReplicas\": 1, \"portRange\": { \"start\": 20000, \"end\": 21000 }, \"maxSurge\": \"10%\", \"spec\": { \"terminationGracePeriod\": \"100\", \"containers\": [ { \"name\": \"game-container\", \"image\": \"<your-game-image-here>\", \"imagePullPolicy\": \"IfNotPresent\", \"command\": [ <required-commands-for-game-image> \"sh example.sh\" ], \"environment\": [ <required-commands-for-game-image> { \"name\": \"EXAMPLE_NAME\", \"value\": \"EXAMPLE_VALUE\" } ], \"requests\": { \"memory\": \"100Mi\", \"cpu\": \"100m\" }, \"limits\": { \"memory\": \"200Mi\", \"cpu\": \"200m\" }, \"ports\": [ { \"name\": \"port-name\", \"protocol\": \"tcp\", \"port\": 12345 } ] } ] }, \"forwarders\": [] }' After running this command, a scheduler will be created with 1 game room replica. Then you can use the following command to get the scheduler details such as how many rooms are ready or occupied: curl --location --request GET \"<maestro-url>/schedulers/info?game=<your-game-name-here>\" \\ --header \"Accept: application/json\" \\ --header \"Authorization: Basic <user:pass in base64>\"","title":"Getting Started"},{"location":"tutorials/GettingStarted/#getting-started-guide","text":"","title":"Getting Started Guide"},{"location":"tutorials/GettingStarted/#prerequisites","text":"Have a game room container image","title":"Prerequisites"},{"location":"tutorials/GettingStarted/#learning-outcomes","text":"After finishing this tutorial you will understand how: to set up your game room to communicate its health status with maestro to configure a new scheduler in maestro with a fixed number of replicas","title":"Learning Outcomes"},{"location":"tutorials/GettingStarted/#configuring-your-game-room","text":"For Maestro to be able to manage your game rooms, you need to ensure that your game room sends a periodic heartbeat to maestro. This heartbeat is what we call ping , in which the room is able to inform its status (such as ready or occupied ) to maestro. For this, you can use maestro-client sdk, if you are using unity, or you can call Maestro rooms API directly using two env vars that are configured in every game room managed by maestro by default. PUT scheduler/$MAESTRO_SCHEDULER_NAME/rooms/$MAESTRO_ROOM_ID/ping { \"status\": \"ready\", \"timestamp\": \"12312312313\" } The status field can be: - ready : the room is ready to accept players. - occupied : the room is occupied by one or more matches, and is not ready to accept more players. - terminating : the room is terminating, and will not accept any new players.","title":"Configuring your game room"},{"location":"tutorials/GettingStarted/#create-a-scheduler","text":"Use the command below to create a new scheduler, this will make a POST request for /schedulers endpoint. You need to change some parameters according to your game room image needs, for further details on all scheduler fields check the reference : image : your game room image. game : your game name (a same game can have multiple schedulers). name : your scheduler name (usually, the stack name). spec.command : any command that is required to run your game room. spec.environment : any environment variable required to run your game room. spec.ports : any port that must be exposed for clients to connect to the game room curl --request POST \\ --url https://<maestro-url>/schedulers \\ --header 'Content-Type: application/json' \\ --header \"Authorization: Basic <user:pass in base64>\" \\ --data '{ \"name\": \"<your-scheduler-name-here>\", \"game\": \"<your-game-name-here>\", \"roomsReplicas\": 1, \"portRange\": { \"start\": 20000, \"end\": 21000 }, \"maxSurge\": \"10%\", \"spec\": { \"terminationGracePeriod\": \"100\", \"containers\": [ { \"name\": \"game-container\", \"image\": \"<your-game-image-here>\", \"imagePullPolicy\": \"IfNotPresent\", \"command\": [ <required-commands-for-game-image> \"sh example.sh\" ], \"environment\": [ <required-commands-for-game-image> { \"name\": \"EXAMPLE_NAME\", \"value\": \"EXAMPLE_VALUE\" } ], \"requests\": { \"memory\": \"100Mi\", \"cpu\": \"100m\" }, \"limits\": { \"memory\": \"200Mi\", \"cpu\": \"200m\" }, \"ports\": [ { \"name\": \"port-name\", \"protocol\": \"tcp\", \"port\": 12345 } ] } ] }, \"forwarders\": [] }' After running this command, a scheduler will be created with 1 game room replica. Then you can use the following command to get the scheduler details such as how many rooms are ready or occupied: curl --location --request GET \"<maestro-url>/schedulers/info?game=<your-game-name-here>\" \\ --header \"Accept: application/json\" \\ --header \"Authorization: Basic <user:pass in base64>\"","title":"Create a scheduler"}]}